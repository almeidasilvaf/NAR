{
  "hash": "f28872e676004a3cff5ea635eb854112",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute: \n  warning: false\n---\n\n\n# Network modelling and hypothesis testing\n\nIn this lesson, you will learn about common mathematical models to describe\ngraphs and how to use them to analyze properties of your own network data.\nAt the end of this lesson, you will be able to:\n\n- understand the properties and parameters of graphs under the most common \ngraph models;\n- use graph models to generate null distributions for hypothesis testing.\n\nLet's start by loading the packages we will use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # for reproducibility\n\n# Load required packages\nlibrary(here)\nlibrary(igraph)\nlibrary(igraphdata)\nlibrary(tidyverse)\n```\n:::\n\n\n## Mathematical graph models\n\nTo characterize the structure of graphs, multiple mathematical models have been \ndeveloped. These models are used for a variety of purposes, including\nassessing the significance of observed properties, and studying mechanisms\nthat led to some observed properties in a particular data set. In this\nsection, we will explore some of the most commonly used graph models.\n\n### Random graphs\n\nClassical random graph models were established by Erdős and \nRényi [@erdds1959random; @erdHos1960evolution; @erdHos1961strength], and later\nslightly adapted by Gilbert [@gilbert1959random]. In these models, a graph\nwith N nodes is created such that, for each pair of nodes, there exists\nan edge with probability between 0 and 1.\n\nIn __igraph__, these graphs can be simulated with the function \n`sample_gnp()`, and you only need to specify the number of nodes in the\ngraph, and the probability for drawing an edge between two arbitrary nodes.\nTo demonstrate it, let's create a random graph according to the \nErdős-Rényi model with 100 nodes and connection probability of 0.02.\nWe will also plot the degree distribution of this graph.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Simulate a random graph (Erdős-Rényi model) with p = 0.02\ng_er <- sample_gnp(100, 0.02)\n\nplot(g_er, layout = layout_in_circle, vertex.label = NA)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n\n```{.r .cell-code}\nhist(degree(g_er), xlab = \"Degree\", ylab = \"Frequency\", main = \"\")\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-2-2.png){width=576}\n:::\n:::\n\n\nIn a classical random graph G with $N$ nodes, for large $N$, \nthe degree distribution will be approximated by a Poisson distribution \nwith mean $c$, where $c = pN$. This is quite intuitive, considering that\nthe degree of any node is distributed as a binomial random variable with\nparameters $N-1$ and $p$. For example, in the graph we simulated above,\nthe expected mean degree is $c = (100-1) \\times 0.02 = 1.98$. Let's verify\nis this is indeed what we observe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate mean degree of the simulated random graph\nmean(degree(g_er))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.88\n```\n\n\n:::\n:::\n\n\nWe can see that the mean degree is indeed very close to the expected value.\nOther properties of classical random graphs include their small average \nshortest-path distances, and low clustering coefficient. Let's verify that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate average shortest-path distance and diameter\nmean_distance(g_er)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.065434\n```\n\n\n:::\n\n```{.r .cell-code}\ndiameter(g_er)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate clustering coefficient\ntransitivity(g_er)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n\n### Practice\n\nSimulate three classical random graphs with 500 nodes and p = 0.02,\n0.05, and 0.1, respectively. Then, answer the questions below:\n\n1. What do their degree distributions look like?\n\n2. Are mean degrees of all graphs close to the expected values?\n\n::: {.callout appearance=\"minimal\" collapse=\"true\"}\n\n### Show me the solutions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Q1\ng1 <- sample_gnp(500, 0.02)\ng2 <- sample_gnp(500, 0.05)\ng3 <- sample_gnp(500, 0.1)\n\nhist(degree(g1))\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(degree(g2))\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(degree(g3))\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Q2\ndegree_df <- data.frame(\n    expected = 499 * c(0.02, 0.05, 0.1),\n    observed = c(mean(degree(g1)), mean(degree(g2)), mean(degree(g3)))\n)\ndegree_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  expected observed\n1     9.98    9.724\n2    24.95   24.804\n3    49.90   49.772\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n### Generalized random graphs\n\nThe graph model proposed by Erdős-Rényi consists in assigning a common \ncharacteristic to a collection of graphs, that is, that the size of the\ngraphs G be equal to some fixed $N_e$. This formulation can be easily \ngeneralized by changing the characteristic that should be fixed in all\ngraphs of a collection. Besides network size, the most common characteristic\nthat researchers choose to set to fixed is the degree sequence. This means\nthat we can create graphs that are different, but have the same degree\nsequence. \n\nIn __igraph__, you can create graphs with a fixed degree sequence using\nthe function `sample_degseq()`. To demonstrate it, let's create a graph with\n8 nodes, with half of the nodes having degree $d = 2$, and the other half\nwith degree $d = 3$.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Define degree sequence\ndseq <- c(rep(2, 4), rep(3, 4))\n\n# Create two graphs with the same pre-defined degree sequence\ng1 <- sample_degseq(dseq, method = \"vl\")\ng2 <- sample_degseq(dseq, method = \"vl\")\n\nplot(g1)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-6-1.png){width=576}\n:::\n\n```{.r .cell-code}\nplot(g2)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-6-2.png){width=576}\n:::\n:::\n\n\nNote that, although the degree sequences are the same, these graphs are\ndifferent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Are graphs isomorphic (i.e., equal)?\nisomorphic(g1, g2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nIt is also important to keep in mind that, while the the degree sequence is\nfixed, all other network characteristics are free to vary from one graph\nto the other. For example, let's create a graph that has the same\ndegree sequence of the yeast protein-protein interaction network, \nand then compare some properties of the original and the simulated networks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get original network\ndata(yeast)\n\n# Simulate graph with the same degree sequence\nyeast_sim <- sample_degseq(degree(yeast), method = \"vl\")\n\n# Compare properties\nprop_comp <- data.frame(\n    mean_degree = c(mean(degree(yeast)), mean(degree(yeast_sim))),\n    diam = c(diameter(yeast), diameter(yeast_sim)),\n    cc = c(transitivity(yeast), transitivity(yeast_sim))\n)\n\nprop_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mean_degree diam         cc\n1    9.059992   15 0.46861779\n2    9.059992    8 0.04005188\n```\n\n\n:::\n:::\n\n\nAs we can see, although the mean degree (and the entire degree distribution)\nis the same, the diameter and clustering coefficient of the graphs\nare completely different.\n\n::: {.callout-tip}\n\n### Practice\n\nUse the code below to load a network containing interactions between\namino acids in the immunoglobulin protein.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(immuno)\n```\n:::\n\n\nThen, simulate a network that has the same degree sequence of the `immuno`\nnetwork and compare the original and the simulated network in terms\nof their:\n\n- mean degree\n- mean betweenness\n- diameter\n- clustering coefficient\n- average shortest-path distances\n\nWhat properties are the same and what properties are different?\n\n::: {.callout appearance=\"minimal\" collapse=\"true\"}\n\n### Show me the solutions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get original network\ndata(immuno)\n\n# Simulate graph with the same degree sequence\nimmuno_sim <- sample_degseq(degree(immuno), method = \"vl\")\n\n# Compare properties\nprop_comp <- data.frame(\n    mean_degree = c(mean(degree(immuno)), mean(degree(immuno_sim))),\n    mean_betweenness = c(mean(betweenness(immuno)), mean(betweenness(immuno_sim))),\n    diam = c(diameter(immuno), diameter(immuno_sim)),\n    cc = c(transitivity(immuno), transitivity(immuno_sim)),\n    sp = c(mean_distance(immuno), mean_distance(immuno_sim)),\n    row.names = c(\"original\", \"simulated\")\n)\n\nprop_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          mean_degree mean_betweenness diam          cc        sp\noriginal     9.574468         9873.342   34 0.485116612 16.016490\nsimulated    9.574468         1607.125    5 0.005932897  3.444296\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n### Small-world models\n\nAs the field of network science evolved, researchers started to focus\non developing models that could more accurately describe real-world networks.\nThe first of this family of models is the 'small-world' network model proposed\nby Watts and Strogatz [@watts1998collective], who observed that many networks\nin the real world display high levels of clustering, but small distances\nbetween most nodes. \n\nTo make a network graph with both of these features, \nWatts and Strogatz suggested starting with a lattice-like graph, then randomly changing (or 'rewiring') a few connections. \nHere's how it works: we begin with a set of $N$ nodes, arranged in a \nperiodic fashion, and join each node to $r$ of its neighbors to each side. \nThen, for each edge, independently and with probability p, one end of that \nedge will be moved to be incident to another node, where that new node is \nchosen uniformly, but with attention to avoid the construction of loops and multi-edges.\n\nIn __igraph__, small-world networks can be simulated using the function\n`sample_smallworld()`. To demonstrate it, let's create a small-world\nnetwork with $N = 25$ nodes, neighborhoods of size $r = 5$, and rewiring\nprobability $p = 0.05$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate small-world network with N = 25, r = 5, p = 0.05\ng_sw <- sample_smallworld(1, 25, 5, 0.05)\nplot(g_sw, vertex.label = NA)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nNow, to understand the effect of rewiring the initial lattice, let's create \nthe lattice alone and explore its properties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create initial lattice alone (N = 100 nodes)\ng_lattice <- sample_smallworld(1, 100, 5, 0)\n\n# Explore lattice's properties\nlist(\n    cc = transitivity(g_lattice),\n    diameter = diameter(g_lattice),\n    sp_distances = mean_distance(g_lattice)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cc\n[1] 0.6666667\n\n$diameter\n[1] 10\n\n$sp_distances\n[1] 5.454545\n```\n\n\n:::\n:::\n\n\nWe can that, in the initial lattice, the clustering coefficient is high,\nbut the network's diameter and average shortest-path distance is also quite\nconsiderable. To resemble real-world networks (with small distances between \nmost nodes), Watts and Strogatz introduced the edge rewiring, which aims\nat reducing the distance between nodes while preserving the clustering \ncoefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Introduce rewiring with p = 0.05 to the initial lattice\ng_rewired <- sample_smallworld(1, 100, 5, 0.05)\n\nlist(\n    cc = transitivity(g_rewired),\n    diameter = diameter(g_rewired),\n    sp_distances = mean_distance(g_rewired)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cc\n[1] 0.5149648\n\n$diameter\n[1] 5\n\n$sp_distances\n[1] 2.777778\n```\n\n\n:::\n:::\n\n\nAs we can observe, rewiring edges with probability $P = 0.05$ leads to a \nmuch smaller diameter and average shortest-path distances. But what happens if\nwe increase the probability $P$? To explore that, let's simulate small-world\nnetworks with $N = 1000$ nodes, $r = 10$, and increasingly large probability\n$P$. For each value of $P$, we will simulate 50 networks and calculate\nthe mean clustering coefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define sequence of P and number of replications per P\nsteps <- seq(-4, -0.5, 0.1)\np <- 10^steps\nnreps <- 50\n\n# Simulate networks and get mean clustering coefficient in 50 replications\ncc_sim <- sapply(p, function(x) {\n    cc <- sapply(seq_len(nreps), function(y) {\n        transitivity(sample_smallworld(1, 1000, 10, x))\n    })\n    return(mean(cc))\n})\n\n# Visualize results\npdata <- data.frame(p = p, cc = cc_sim / max(cc_sim))\nggplot(pdata, aes(x = p, y = cc)) +\n    geom_line() +\n    theme_bw() +\n    scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe figure shows that the clustering coefficient remains high for a \nconsiderable range of $P$.\n\n::: {.callout-tip}\n\n### Practice\n\nRepeat the same simulation as above, but instead of calculating the mean\nclustering coefficient in 50 networks for each value $P$, calculate\nthe mean average shortest-path distance. \n\nThen, recreate the line plot above, but now with two lines (in different \ncolors) indicating how clustering coefficients and average shortest-path\ndistances vary as a function of $P$. What do you conclude?\n\n::: {.callout appearance=\"minimal\" collapse=\"true\"}\n\n### Show me the solutions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate networks and get mean clustering coefficient in 50 replications\nsp_sim <- sapply(p, function(x) {\n    sp <- sapply(seq_len(nreps), function(y) {\n        mean_distance(sample_smallworld(1, 1000, 10, x))\n    })\n    return(mean(sp))\n})\n\n# Visualize results\npdata <- data.frame(\n    p = rep(p, 2),\n    stat = c(cc_sim / max(cc_sim), sp_sim / max(sp_sim)),\n    line = rep(c(\"CC\", \"SP\"), each = length(p))\n)\n\nggplot(pdata, aes(x = p, y = stat)) +\n    geom_line(aes(color = line)) +\n    theme_bw() +\n    scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n\n\n### Preferential attachment models\n\nOver the past few decades, network scientists have tried to develop\nmodels that could mimic how networks grow over time. Classical examples of \nnetworks that evolve over time are the World Wide Web, citation networks,\nand biological networks (e.g., protein-protein interaction and gene \nregulatory networks). As pioneers in this field, @barabasi1999emergence were intrigued by the growth of the World Wide\nWeb, and they noticed that pages that are cited by many other pages tend to\naccumulate increasingly greater number of links over time. This property,\nnow widely known as the 'rich get richer' principle, means that the more\nedges a node has, the more likely it is to gain even more edges over time.\n\n\nModels based on this assumption are called *preferential attachment* models,\nwith the Barabási-Albert (BA) model arguably being the most famous one.\nIn the BA model, we start with a graph $G$ with $N_n$ nodes and $N_e$ edges.\nThen, at each stage $t = 1,2,...$, the current graph $G_t$ is modified\nto create a new graph $G_{t+1}$ by adding a new node of degree $m \\ge 1$,\nwhich means that the new node will have $m$ connections to $m$ different\nnodes in $G_t$. The probability that the new node will be connected to any\nnode $i$ is\n\n$$\np_i = \\frac{k_i}{\\sum_j k_j}\n$$\n\nwhere $k_i$ is the degree of node $i$ and the sum is made over all\nnodes in graph $G_t$. This indicates that new nodes are connected to existing\nnodes in a manner preferential to those with highest degrees.\n\nIn __igraph__, we can simulate a network under the BA model using the\nfunction `sample_pa()`. To demonstrate it, let's create a graph with 100 nodes\nand $m = 1$ new edges added for each node. Let's also plot the simulated graph\nand its degree distribution.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Create a network with N = 100 and m = 1 under the BA model\ng_ba <- sample_pa(100, m = 1, directed = FALSE)\n\n# Plot graph (circular layout) and degree distribution\nplot(g_ba, layout = layout_in_circle, vertex.label = NA)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-16-1.png){width=576}\n:::\n\n```{.r .cell-code}\nhist(degree(g_ba), xlab = \"Degree\", ylab = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-16-2.png){width=576}\n:::\n:::\n\n\nAs we can see on the plots, there is a small number of nodes with very high\ndegree (the so-called 'hubs'), while most nodes have very low degree. Let's \ntake a look at a quick summary of the degree distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(degree(g_ba))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    1.00    1.98    2.00   10.00 \n```\n\n\n:::\n:::\n\n\nMost nodes have degree of no more than two, while some nodes \nhave (comparatively) very large degrees.\n\nA major difference between preferential attachment models and classical\nrandom graphs is that, as the number of stages $t$ tend to infinity, the\ndegree distributions of the graphs tend to a power-law form $d^{-\\alpha}$,\nwith $\\alpha=3$. Nevertheless, the BA model shares some similarities with\nits classical counterparts, such as the tendency to generate networks\nwith small average shortest-path distances and low clustering coefficient. Let's\nverify that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_distance(g_ba)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.806869\n```\n\n\n:::\n\n```{.r .cell-code}\ndiameter(g_ba)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n\n```{.r .cell-code}\ntransitivity(g_ba)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n## Assessing the significance of graph features\n\nFrom a statistical hypothesis testing perspective, the models described in\nthe previous sections are very useful to assess the significance of network\ngraph characteristics. For instance, suppose we have a graph inferred from data\nand we are interested in some structural characteristic (e.g., clustering\ncoefficient, number of communities, etc). A standard analysis is to investigate\nwhether the observed characteristic is 'significant' (i.e., more extreme\nthan the expected by chance). In this context, we can use the network models\ndescribed above are used to create a reference distribution to which we compare\nour observed values. Below, we will demonstrate how this works with some use \ncases.\n\n### Use case 1: number of communities in a network\n\nIn the previous chapter, we explored community detection algorithms using the\nkarate club network as an example data set. Using the greedy optimization of\nmodularity algorithm (in the function `cluster_fast_greedy()`), we identify\nthree communities in this network, as demonstrated below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get karate club network and detect communities with infomap\ng <- make_graph(\"Zachary\")\n\nlength(cluster_fast_greedy(g))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\nA natural question we might ask ourselves is whether this number is somehow\nunexpected. To test this hypothesis, we will use Monte Carlo methods to compare\nthe observed number of communities to a reference distribution consisting\nof random graphs of same order ($N_n = 34$ nodes) and size ($N_e$ = 78 edges)\nas the karate network. Let's first extract the order and size of the original\nnetwork.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get number of nodes and edges\nnn <- vcount(g)\nne <- ecount(g)\n```\n:::\n\n\nNext, over 1000 trials, we will generate classical random graphs of same\norder and size, and detect communities using the greedy optimization\nof modularity algorithm.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 1000 classical random graphs and detect communities\nnull_comm <- sapply(seq_len(1000), function(x) {\n    rg <- sample_gnm(nn, ne)\n    ncomm <- length(cluster_fast_greedy(rg))\n    return(ncomm)\n})\n```\n:::\n\n\nNow, let's visualize the probability of finding each number of communities\nin our random networks of same order and size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize nulls\ncounts <- table(null_comm) / 1000\nbarplot(counts)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThe figure shows that it's very unlikely to find the number of communities\nwe observed from the perspective of random graphs of fixed size.\n\n::: {.callout-tip}\n\n### Practice\n\nIn the example above, we assessed the significance of the observed number\nof communities detected with `cluster_fast_greedy()` by comparing it\nto a null distribution of communities detected from classical random graphs.\nRepeat the same procedure, but now use generalized random graphs constrained\nto have the same degree distribution as the original karate club network.\n\nIs the observed number of communities also significant using this alternative\nmodel?\n\n::: {.callout appearance=\"minimal\" collapse=\"true\"}\n\n### Show me the solutions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find communities in 1000 random graphs of same degree distribution\ndegs <- degree(g)\nnull_comm2 <- sapply(seq_len(1000), function(x) {\n    grg <- sample_degseq(degs, method = \"vl\")\n    ncomm <- length(cluster_fast_greedy(grg))\n    return(ncomm)\n})\n\ncounts2 <- table(null_comm2) / 1000\nbarplot(counts2)\n```\n\n::: {.cell-output-display}\n![](03_network_modeling_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n\n### Use case 2: small-world properties\n\nMany neuroscientists have reported that small-world properties can be\nobserved in network-based representations of the brain. Recall that \nsmall-world networks have high levels of clustering, but small distances\nbetween most nodes. Hence, similarly to what we did in our previous use case,\nwe could compare the observed clustering coefficient and average \nshortest-path distances to null distributions derived from classical random\ngraphs.\n\nTo demonstrate this, we will use the `macaque` data set, which contains a\ndirected network (45 nodes and 463 edges) of established functional \nconnections between brain areas involved in the tactile function of the \nvisual cortex in macaque monkeys [@negyessy2006prediction].\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data and take a quick look at it\ndata(macaque)\nsummary(macaque)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIGRAPH f7130f3 DN-- 45 463 -- \n+ attr: Citation (g/c), Author (g/c), shape (v/c), name (v/c)\n```\n\n\n:::\n:::\n\n\nTo calculate clustering coefficients, we will use an extension of the method\nimplemented in `transitivity()` that is more suitable for directed graphs,\nimplemented in the function below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define function to calculate clustering coefficient for directed graphs\ncc_dir <- function(graph) {\n    A <- as.matrix(as_adjacency_matrix(graph))\n    S <- A + t(A)\n    deg <- degree(graph, mode = \"total\")\n    num <- diag(S %*% S %*% S)\n    denom <- diag(A %*% A)\n    denom <- 2 * (deg * (deg - 1) - 2 * denom)\n    cl <- mean(num / denom)\n    \n    return(cl)\n}\n```\n:::\n\n\nThen, over 1000 trials, we will simulate directed random graphs and get\nnull distributions of clustering coefficients and average shortest-path lengths.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define parameters\nnn <- vcount(macaque)\nne <- ecount(macaque)\n\n# Generate null distributions and store them in a 2-column data frame\nnulls <- lapply(seq_len(1000), function(x) {\n    rg <- sample_gnm(nn, ne, directed = TRUE)\n    df <- data.frame(cc = cc_dir(rg), sp = mean_distance(rg))\n})\nnulls <- Reduce(rbind, nulls)\n```\n:::\n\n\nNext, let's compare our observed values to the null distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observed values\nlist(cc = cc_dir(macaque), sp = mean_distance(macaque))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cc\n[1] 0.5501073\n\n$sp\n[1] 2.148485\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary of nulls\nsummary(nulls$cc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2166  0.2304  0.2341  0.2341  0.2378  0.2513 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(nulls$sp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.814   1.828   1.833   1.833   1.838   1.863 \n```\n\n\n:::\n:::\n\n\nWe can see that our observed values fall far outside the range of these random\ngraphs. The observed clustering coefficient is higher than expected from\na random network. However, the average shortest-path distance is also\ngreater than expected, which is not what we expect from a small-world network.\nHence, we do not have very strong evidence in support of small-world properties\nin this network.\n\n## Session information {.unnumbered}\n\nThis chapter was created under the following conditions:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Brussels\n date     2024-04-19\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.2)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.2)\n digest        0.6.34  2024-01-11 [1] CRAN (R 4.3.2)\n dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.3.2)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.2)\n fansi         1.0.6   2023-12-08 [1] CRAN (R 4.3.2)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.2)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.2)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.2)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.2)\n ggplot2     * 3.5.0   2024-02-23 [1] CRAN (R 4.3.2)\n glue          1.7.0   2024-01-09 [1] CRAN (R 4.3.2)\n gtable        0.3.4   2023-08-21 [1] CRAN (R 4.3.2)\n here        * 1.0.1   2020-12-13 [1] CRAN (R 4.3.2)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.2)\n htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.3.2)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.2)\n igraph      * 2.0.1.1 2024-01-30 [1] CRAN (R 4.3.2)\n igraphdata  * 1.0.1   2015-07-13 [1] CRAN (R 4.3.2)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.2)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.3.2)\n labeling      0.4.3   2023-08-29 [1] CRAN (R 4.3.2)\n lattice       0.22-5  2023-10-24 [4] CRAN (R 4.3.1)\n lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.3.2)\n lubridate   * 1.9.3   2023-09-27 [1] CRAN (R 4.3.2)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.2)\n Matrix        1.6-3   2023-11-14 [4] CRAN (R 4.3.2)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.2)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.2)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.2)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.2)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.2)\n readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.3.2)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.2)\n rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.3.2)\n rprojroot     2.0.4   2023-11-05 [1] CRAN (R 4.3.2)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.2)\n scales        1.3.0   2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.2)\n stringi       1.8.3   2023-12-11 [1] CRAN (R 4.3.2)\n stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.3.2)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.2)\n tidyr       * 1.3.1   2024-01-24 [1] CRAN (R 4.3.2)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.2)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.2)\n timechange    0.3.0   2024-01-18 [1] CRAN (R 4.3.2)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.2)\n utf8          1.2.4   2023-10-22 [1] CRAN (R 4.3.2)\n vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.3.2)\n withr         3.0.0   2024-01-16 [1] CRAN (R 4.3.2)\n xfun          0.42    2024-02-08 [1] CRAN (R 4.3.2)\n yaml          2.3.8   2023-12-11 [1] CRAN (R 4.3.2)\n\n [1] /home/faalm/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n## References {.unnumbered}\n\n",
    "supporting": [
      "03_network_modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}